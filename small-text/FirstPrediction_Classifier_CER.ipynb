{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c260b-ea73-4cd8-930a-13bc75b8923e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install small-text[transformers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-abuse",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install setfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17587b6-2513-4a3c-980a-b7269bee14fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ae34a2-4677-4cc7-8396-b84aa3396236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed510375-2139-4b99-ab6e-ee6d92439d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the rcParams module from the matplotlib library to customize plot appearance\n",
    "from matplotlib import rcParams\n",
    "# Update the font size of the x-axis and y-axis labels, as well as the overall axis labels\n",
    "rcParams.update({'xtick.labelsize': 14, 'ytick.labelsize': 14, 'axes.labelsize': 16})\n",
    "\n",
    "# Import the torch and numpy libraries for numerical computations\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Set a seed value for reproducibility in random number generation\n",
    "seed = 2022\n",
    "torch.manual_seed(seed)  # Set the seed for PyTorch\n",
    "np.random.seed(seed)     # Set the seed for NumPy\n",
    "\n",
    "# Import the logging, glob, os, and pandas libraries for file handling and data manipulation\n",
    "import logging\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Read the initial training data from a CSV file into a pandas DataFrame\n",
    "#initial_data = pd.read_csv(\"initial_train_updated.CSV\", delimiter='\\t', index_col=False)\n",
    "initial_data = pd.read_csv(\"initial_train_smalltext.csv\", delimiter='\\t', index_col=False)\n",
    "print(initial_data)  # Print the initial training data\n",
    "\n",
    "# Get the number of unique classes in the 'Label.Marius' column of the initial training data\n",
    "num_classes = initial_data['label'].nunique()\n",
    "\n",
    "print(\"----------------------------\")\n",
    "\n",
    "# Define the path to the 'Training Data' directory\n",
    "path = r'./Training Data'\n",
    "\n",
    "# Use the glob library to get a list of all CSV files in the specified directory and its subdirectories\n",
    "all_files = glob.glob(os.path.join(path, \"**/*.csv\"), recursive=True)\n",
    "\n",
    "# Read and concatenate all the CSV files into a single pandas DataFrame\n",
    "new_data = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n",
    "print(new_data)  # Print the concatenated data\n",
    "\n",
    "print(\"----------------------------\")\n",
    "\n",
    "# Print the first 2 training samples from the initial training data\n",
    "print('First 2 training samples:\\n')\n",
    "for i in range(2):\n",
    "    print(initial_data.iloc[i, 0], ' ', initial_data.iloc[i, 1])\n",
    "\n",
    "# Print the first 2 future samples from the concatenated data\n",
    "print('First 2 future samples:\\n')\n",
    "for i in range(2):\n",
    "    print(new_data.iloc[i, 0], ' ', initial_data.iloc[i, 1])\n",
    "\n",
    "print(\"----------------------------\")\n",
    "\n",
    "# Print the profile attribute of the new_data DataFrame \n",
    "print(new_data.example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00cbe7f-a743-47b6-b8d1-eb1158397953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the numpy library for numerical computations\n",
    "import numpy as np\n",
    "# Import the LABEL_IGNORED and LABEL_UNLABELED constants from the small_text.base module\n",
    "from small_text.base import LABEL_IGNORED, LABEL_UNLABELED\n",
    "\n",
    "# Convert the 'label' column of the initial_data DataFrame into a categorical variable\n",
    "initial_data['label'] = pd.Categorical(initial_data.label)\n",
    "# Create a new column 'code' in the initial_data DataFrame that contains the numerical codes of the categorical labels\n",
    "initial_data['code'] = initial_data['label'].cat.codes\n",
    "\n",
    "# Print the initial_data DataFrame with the new 'code' column\n",
    "print(initial_data)\n",
    "print(\"--------\")\n",
    "\n",
    "# Create a new DataFrame 'df' by converting lists of tuples into a pandas DataFrame\n",
    "# The DataFrame has three columns: 'example', 'label', and 'code'\n",
    "# The 'example' column contains the values from the 'profile' column of the new_data DataFrame\n",
    "# The 'label' column contains the string 'LABEL_UNLABELED' for all rows\n",
    "# The 'code' column contains the numerical value of the LABEL_UNLABELED constant for all rows\n",
    "df = pd.DataFrame(\n",
    "    list(zip(new_data.example,  # use column \"profile\"\n",
    "             ['LABEL_UNLABELED'] * new_data.shape[0],\n",
    "             [LABEL_UNLABELED] * new_data.shape[0])),\n",
    "    columns=['example', 'label', 'code'])\n",
    "\n",
    "# Print the newly created DataFrame 'df'\n",
    "print(df)\n",
    "print(\"--------\")\n",
    "\n",
    "# Concatenate the initial_data and df DataFrames along the row axis\n",
    "# The resulting DataFrame 'input_data' has a multi-level index with two levels: 'Initial Training' and 'Twitter Data'\n",
    "input_data = pd.concat([initial_data, df], keys=['Initial Training', 'Twitter Data'])\n",
    "\n",
    "# Print the concatenated DataFrame 'input_data'\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8280383b-3738-4e7f-8b05-4850f3e79ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE NA FROM  TEXTS\n",
    "input_data.loc[input_data.example.isna(), 'example'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f89def74-e550-4c77-89e6-f5f0bea1ecd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msmall_text\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextDataset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Creates an array of numbers from 0 to num_classes-1. The variable num_classes is presumably defined elsewhere in the code.\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m target_labels \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marange(num_classes)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Prints the array of target labels to the console.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(target_labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Import the TextDataset class from the small_text module.\n",
    "from small_text import TextDataset\n",
    "\n",
    "# Creates an array of numbers from 0 to num_classes-1. The variable num_classes is presumably defined elsewhere in the code.\n",
    "target_labels = np.arange(num_classes)\n",
    "\n",
    "# Prints the array of target labels to the console.\n",
    "print(target_labels)\n",
    "\n",
    "# Creating an instance of the TextDataset using the `from_arrays` class method. This requires several arguments:\n",
    "# 1. A list of examples obtained from the 'example' column of the input_data dataframe.\n",
    "# 2. An array of codes obtained from the 'code' column of the input_data dataframe.\n",
    "# 3. The target labels, which is an array of integers from 0 to num_classes-1.\n",
    "train = TextDataset.from_arrays(\n",
    "    input_data.example.tolist(),\n",
    "    input_data.code.to_numpy(),\n",
    "    target_labels=target_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97487aee-4775-485c-ba3f-34b49cf775f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check max length\n",
    "max_length = 0\n",
    "for example in input_data.example:\n",
    "    length = len(example)\n",
    "    if length > max_length:\n",
    "        max_length = length\n",
    "        \n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62a3ebe-adbe-4447-be42-6255f051fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SetFitModelArguments class from the small_text.integrations.transformers.classifiers.setfit module.\n",
    "# used to define or set up arguments required for a SetFit model.\n",
    "from small_text.integrations.transformers.classifiers.setfit import SetFitModelArguments\n",
    "\n",
    "# Import the SetFitClassificationFactory class from the small_text.integrations.transformers.classifiers.factories module.\n",
    "# provides a mechanism to create or instantiate SetFit classification models based on the provided arguments.\n",
    "from small_text.integrations.transformers.classifiers.factories import SetFitClassificationFactory\n",
    "\n",
    "# Define a string variable that holds the name of a pretrained sentence transformer model.\n",
    "# This particular model, 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', is designed for paraphrasing tasks and supports multiple languages.\n",
    "sentence_transformer_model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "\n",
    "# Create an instance of SetFitModelArguments using the previously defined sentence transformer model name.\n",
    "# encapsulate necessary configuration or settings needed to initialize or train a SetFit model using the specified transformer model.\n",
    "setfit_model_args = SetFitModelArguments(sentence_transformer_model_name)\n",
    "\n",
    "# Create an instance of the SetFitClassificationFactory.\n",
    "# This factory requires two main arguments:\n",
    "# 1. The SetFit model arguments, which were defined in the previous step.\n",
    "# 2. The number of classes (num_classes), which is presumably defined elsewhere in your code.\n",
    "# Additionally, an optional classification_kwargs dictionary is provided, which sets a \"max_seq_len\" (maximum sequence length) of 128 for the classification task.\n",
    "clf_factory = SetFitClassificationFactory(setfit_model_args, \n",
    "                                          num_classes, \n",
    "                                          classification_kwargs=dict({\"max_seq_len\":128}))  # set maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ccd341-c009-4efc-b261-db9834631e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(train.y > -1)[0] #show which profile descriptions have already been labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5831721f-e009-4a46-b716-33ffdcba399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from class_balancer import ClassBalancer #file in workingspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205117d-40e9-4c9e-b30c-903f24b88f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import various classes and functions from the small_text module.\n",
    "from small_text import (\n",
    "    PoolBasedActiveLearner,           # A class for pool-based active learning.\n",
    "    PredictionEntropy,                # A measure or strategy based on the prediction's entropy.\n",
    "    TransformerBasedClassificationFactory,  # Factory to create transformer-based classifiers.\n",
    "    TransformerModelArguments,        # Arguments required for transformer models.\n",
    "    random_initialization_balanced,   # A function/method for balanced initialization of data.\n",
    "    BreakingTies,                     # A query strategy based on breaking ties in predictions.\n",
    "    SEALS                             # Another query strategy.\n",
    ")\n",
    "\n",
    "# imports related to other query strategies and exceptions:\n",
    "# from small_text.query_strategies import PoolExhaustedException, EmptyPoolException\n",
    "# from small_text.query_strategies import RandomSampling, LeastConfidence, PredictionEntropy\n",
    "\n",
    "# Define a function to initialize an active learner.\n",
    "# This function simulates an initial labeling to kick-start the active learning process.\n",
    "def initialize_active_learner(active_learner, y_train):\n",
    "    \n",
    "    # Find indices where y_train has values greater than -1.\n",
    "    # labeled instances.\n",
    "    indices_initial = np.where(y_train > -1)[0]\n",
    "    \n",
    "    # Provide the initial labeled data to the active learner.\n",
    "    active_learner.initialize_data(indices_initial, y_train[indices_initial])\n",
    "    \n",
    "    # Print the labels of the initial labeled data.\n",
    "    print(y_train[indices_initial])\n",
    "    \n",
    "    return indices_initial\n",
    "\n",
    "# Set the active learning query strategy.\n",
    "query_strategy = SEALS(BreakingTies())  # or also just \"BreakingTies()\"\n",
    "# other strategies that might be used:\n",
    "# query_strategy = ClassBalancer(PredictionEntropy(), ignored_classes=[10], subsample_size=None)\n",
    "# query_strategy = PredictionEntropy()\n",
    "\n",
    "# Arguments for training the SetFit model.\n",
    "# Here, it's set to show a progress bar during training.\n",
    "setfit_train_kwargs = {'show_progress_bar': True}\n",
    "\n",
    "# Create an instance of the PoolBasedActiveLearner.\n",
    "# It requires:\n",
    "# - a factory to create classifiers (clf_factory, presumably defined in previous snippets)\n",
    "# - a query strategy for active learning\n",
    "# - the training data\n",
    "# - training arguments specific to the SetFit model\n",
    "active_learner = PoolBasedActiveLearner(\n",
    "    clf_factory, \n",
    "    query_strategy, \n",
    "    train, \n",
    "    fit_kwargs={'setfit_train_kwargs': setfit_train_kwargs}\n",
    ")\n",
    "\n",
    "# Initialize the active learner with the training labels.\n",
    "# Store the indices of the labeled data points.\n",
    "indices_labeled = initialize_active_learner(active_learner, train.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318bb0d2-42eb-4377-b80e-aafd158cb2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and tools.\n",
    "# `accuracy_score` and `f1_score` are metrics used to evaluate the performance of a machine learning model.\n",
    "# `clear_output` is a utility function from IPython to clear the cell's output in Jupyter Notebooks.\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Set the number of queries for the active learning process. Currently, it is set to 1.\n",
    "num_queries = 1\n",
    "\n",
    "# Define a function called 'evaluate' to assess the performance of the active learner.\n",
    "# The function will evaluate the learner on training data using the F1 score.\n",
    "def evaluate(active_learner, train_set): \n",
    "\n",
    "    # Use the classifier within the active_learner to predict the labels of the train_set.\n",
    "    y_pred = active_learner.classifier.predict(train_set)\n",
    "    \n",
    "    # This line is commented out, but if uncommented, it would predict labels for a test set.\n",
    "    # y_pred_test = active_learner.classifier.predict(test)\n",
    "    \n",
    "    # Print the predicted labels.\n",
    "    print(y_pred)\n",
    "    \n",
    "    # Print the true labels of the training data.\n",
    "    print(train_set.y)\n",
    "\n",
    "    # Compute and print the F1 score for the predicted labels versus the true labels.\n",
    "    # The F1 score is computed using macro averaging, which calculates metrics for each label and finds their unweighted mean.\n",
    "    print('Train f1: {:.2f}'.format(\n",
    "        f1_score(y_pred, train_set.y, average='macro')))\n",
    "    print('---')\n",
    "\n",
    "# Create an empty list to store results.\n",
    "results = []\n",
    "\n",
    "# These lines are commented out, but they would evaluate the active learner and append the results.\n",
    "# There seems to be an inconsistency in the arguments passed to the 'evaluate' function in the commented lines.\n",
    "# results.append(evaluate(active_learner, train[indices_labeled], test))\n",
    "# results.append(evaluate(active_learner, train[indices_labeled]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d9b6d1-d54b-43df-9a08-7bc3ece7dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first prediction\n",
    "\n",
    "import csv\n",
    "\n",
    "# Query the active learner to get indices of the top 70 samples which it wants to label.\n",
    "indices_queried = active_learner.query(num_samples=70)\n",
    "\n",
    "# Use the classifier within the active_learner to predict the class probabilities for the queried samples.\n",
    "probas = active_learner.classifier.predict_proba(train[indices_queried])\n",
    "\n",
    "# Convert the predicted class probabilities to class labels by selecting the class with the highest probability.\n",
    "y_predicted = np.argmax(probas, axis=1)\n",
    "\n",
    "# Initialize an empty list called y\n",
    "y = []\n",
    "\n",
    "# Print the predicted labels.\n",
    "print(y_predicted)\n",
    "\n",
    "# Open a new CSV file to write the predicted labels.\n",
    "with open('Predicted Labels/predicted_labels_cer_1.csv', mode='w', newline='') as file:\n",
    "    # Define the column names or fieldnames for the CSV.\n",
    "    fieldnames = ['text', 'prediction', 'true label']\n",
    "    \n",
    "    # Use the csv.DictWriter to write dictionaries into the CSV file.\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    \n",
    "    # Write the header row using the defined fieldnames.\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Iterate over the queried indices and write each sample's text, predicted label, and a placeholder for the true label.\n",
    "    for j in range(len(indices_queried)):\n",
    "        writer.writerow({'text': train.x[indices_queried[j]], 'prediction': y_predicted[j], 'true label': \"\"})\n",
    "\n",
    "# Print the y list, which remains empty at this point.\n",
    "print(y)\n",
    "\n",
    "# Pause the program execution and prompt the user to press Enter to continue.\n",
    "input(\"Press Enter to continue...\")\n",
    "\n",
    "# The following commented line simulates user interaction.\n",
    "# In a real-world scenario, after writing the predicted labels to the CSV, an expert might manually label them.\n",
    "# After this, the true labels would replace the placeholder values in the CSV.\n",
    "# y = train.y[indices_queried]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4212a98-4162-496c-8ca6-a37e68a3b7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: pickle the model\n",
    "import pickle\n",
    "\n",
    "with open('active_learner_cer.pkl', 'wb') as file:\n",
    "    pickle.dump(active_learner, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd7f81c-be4a-43af-99e0-c20b74e148d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## test\n",
    "import pickle \n",
    "\n",
    "with open('active_learner_cer.pkl', 'rb') as file:\n",
    "    active_learner = pickle.load(file)\n",
    "    \n",
    "print(active_learner.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d66aaa-40e7-434e-8bca-a6e419ff5db2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
